{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<pre><code>                           CttattA          AtttaT    aaaatt                 tttt\n                            ttttt          Aatt       attttG                 Gttt\n                            ttttt         Cta         GttttC                 Gttt\n                            ttttt       CaG           GttttC                 Gtta\n                            ttttt     CaG             GttttC                 Ctta\n                            ttttt   CaC               GttttC                 Ctta\n                            ttttt TatA                GttttC                 Ctta\n                            tttttGtttaT               GttttC                 Ctta\n                            ttttt  ttttaT             GttttC                 Ctta\n                            ttttt    ttttaA           GttttC                 GttG\n                            ttttt     AttttGA         TttttG                 attT\n                            ttttt       AttttGA        tttta                 ttt\n                            ttttt         TttttG       CttttC               TttT\n                            ttttt           CttttG      CttttT             TatT\n                           Tttttt             atttaC      atttaT         TGtT\n                          ATatttttGAAA           ttttaC       AtttaGGGGGat\n                                                  ttttaC\n                                                    ttttaGA\n                                                        GttaGCTA\n</code></pre>"},{"location":"#genomic-data-science-core","title":"Genomic Data Science Core","text":"<p>We are a data analysis and training service for Kansas researchers, based at the University of Kansas and supported by the Kansas INBRE and the KU Center for Genomics.</p> <ul> <li>People</li> <li>Policies</li> <li>Services Offered</li> <li>Documentation and Training</li> </ul>"},{"location":"#contact","title":"Contact","text":"<p>gdsc@ku.edu</p>"},{"location":"people/","title":"People","text":"Brian J. Sanderson, Ph.D.  Stuart J. Macdonald, Ph.D.  Robert L. Unckless, Ph.D. Lead Genomic Data Scientist  Department of Molecular  Biosciences / Center for Genomics   University of Kansas  Lawrence, KS 66045  brian.sanderson@ku.edu  Director, Genomic Data Science Core  Department of Molecular Biosciences  University of Kansas  Lawrence, KS 66045  sjmac@ku.edu Director, KU Center for Genomics  Department of Molecular Biosciences  University of Kansas  Lawrence, KS 66045  unckless@ku.edu"},{"location":"policies/","title":"Policies","text":""},{"location":"policies/#acknowledgement-and-authorship","title":"Acknowledgement and Authorship","text":"<p>Publications, presentations, grants, and other related products resulting from contributions from our service should acknowledge K-INBRE, which is supported by the IDeA Program of the National Institute of General Medical Science award number P20 GM103418, as well as the University of Kansas Center for Genomics.</p> <p>Additionally, products that result from substantial contributions of our staff to experimental design, implementation, analysis and interpretation of resulting data, and/or drafting and revision of manuscripts, should include the relevant contributors as co-authors. Whether a project will entail co-authorship is an important conversation we will have at the outset of new collaborations, pending an assessment of the efforts that will be required.</p>"},{"location":"policies/#grant-applications","title":"Grant Applications","text":"<p>We strongly encourage researchers to contact us at the earliest possible stage in the development of projects. Through these conversations we can help discuss issues related to experimental design and data collection that may impact downstream analysis. These discussions will also help to clarify our estimates of the extent of our contributions to a project, which can result in letters of support or assignment of our staff as funded, grant personnel as necessary.</p>"},{"location":"services/","title":"Services Offered","text":"<p>We are able to assist with a wide range of data analysis problems, such as:</p> <ul> <li>Patterns of gene expression among individuals, tissues, etc.</li> <li>Whether and how gene expression changes in response to experimental stimulus</li> <li>Quantifying the relationships between genetic and phenotypic variation</li> <li>Biomarker discovery</li> <li>Estimation and comparison of rates of sequence evolution</li> </ul> <p>These questions may be addressed with one or more of the following methods:</p> <ul> <li>Processing and preliminary analysis of data from Illumina, Pacific Biosciences, and Oxford Nanopore sequencing data</li> <li>Quantification of counts of gene expression from bulk or single cell RNA sequencing, and subsequent analysis of differential gene expression</li> <li>Discovery of single nucleotide variants, small insertion/deletions, and larger structural variation from whole genome, whole exome, or other target capture sequencing</li> <li>Assembly and annotation of gene and genome sequence data</li> <li>Multiple sequence or genome alignment</li> <li>Imputation of genotypes from variant data</li> </ul> <p>We can provide detailed, reproducible analysis tools, intermediate and final analysis output, and publication-quality figures and tables.</p> <p>Note: we provide data analysis services. If you need to generate sequence data, please contact the Genome Sequencing Core</p>"},{"location":"documentation/","title":"Documentation and Training Materials","text":"<p>Documentation and tutorials for performing analyses using high performance computing clusters:</p> <ul> <li>Quick Start Guides</li> <li>More Detailed Guides</li> <li>Command Cheatsheet</li> </ul> <p>Detailed documentation for analysis workflows:</p> <ul> <li>Workflows<ul> <li>Microbial RNA-seq</li> </ul> </li> </ul>"},{"location":"documentation/cheatsheet/","title":"Generally useful *nix command cheatsheet","text":"<p>whoami Prints the username with which you logged in</p> <p>pwd Prints your 'present working directory.'</p> <p>ls 'Lists' the files in a current directory by itself, or given a path will list the files in that directory.</p> <ul> <li> <p>Useful flags:</p> <ul> <li> <p>-a: list all files, even 'hidden' files</p> </li> <li> <p>-h: make file sizes human readable (e.g. 4.0K instead of 4000)</p> </li> <li> <p>-l: print in tabular 'list' form</p> <p><code>ls -l /home/brian/a_directory/</code></p> <ul> <li>This would print out a list of the files in this directory, regardless of your present working directory.</li> </ul> </li> </ul> </li> </ul> <p>cd 'Change directory' from your pwd to some target.</p> <ul> <li> <p>If the directory is nested within your pwd you can type a relative path. If it is not, you should type an absolute path.</p> <p><code>cd /home/brian/a_directory</code></p> </li> <li> <p>Useful shortcuts for directory locations:</p> <ul> <li> <p>\\~: Your home directory (e.g. /home/USERNAME)</p> </li> <li> <p>..: The directory above the current one (e.g. if you're in /home/USERNAME, .. refers to /home)</p> </li> </ul> </li> </ul> <p>cp Makes a 'copy' of a file and places it in a target directory</p> <ul> <li> <p>This would create a new copy with the same name in a new directory:</p> <p><code>cp filename.txt /home/brian/newDirectory/</code></p> </li> <li> <p>This would create a new copy with a different name in a new directory:</p> <p><code>cp filename.txt /home/brian/newDirectory/differentname.txt</code></p> </li> </ul> <p>mv 'Move' a file from its current location to a new location.</p> <ul> <li> <p>The syntax is similar to cp.</p> </li> <li> <p>This is also a simple way to change the name of a file in its current directory.</p> <p><code>mv filename.txt differentname.txt</code></p> </li> </ul> <p>rm 'Removes' a file now and forever.</p> <ul> <li> <p>Useful flags:</p> <ul> <li> <p>-i interactive mode, requires you to confirm each file deletion.</p> </li> <li> <p>r used with a target directory to delete all of its contents as well as the directory itself</p> </li> </ul> </li> </ul> <p>rmdir 'Removes a directory' now and forever.</p> <ul> <li>The directory must be empty. If you're really sure you want to delete a full directory, use rm -r or perhaps rm -ir as above.</li> </ul> <p>mkdir 'Makes a directory.'</p> <ul> <li> <p>Make a directory called 'newDirectory' in my pwd:</p> <p><code>mkdir newDirectory</code></p> </li> <li> <p>Make a directory called 'newDirectory' in a specified path:</p> <p><code>mkdir /home/brian/specifiedPath/newDirectory/</code></p> </li> </ul> <p>touch Creates an empty file with the specified name. Sometimes useful.</p> <ul> <li> <p>Create a new text file called newFile.txt</p> <p><code>touch newFile.txt</code></p> </li> </ul> <p>chown Changes the ownership of a file.</p> <ul> <li> <p>Change the owner of secretDocument.txt from its current owner to 'brian'</p> <p><code>chown brian secretDocument.txt</code></p> </li> </ul> <p>chgrp Changes the group ownership of a file.</p> <ul> <li> <p>Change the group ownership of secretDocument.txt from its current group to 'trustedGroup'</p> <p><code>chgrp trustedGroup secretDocument.txt</code></p> </li> </ul> <p>chmod Changes the permissions of a file.</p> <ul> <li> <p>There are three sets of permissions for a file: User, Group, and Other. The 'user' is the owner of the file, the 'group' is the group, and 'other' is anyone who is not the owner and not in the group.</p> </li> <li> <p>For each set, there are three permissions: Read, Write, and eXecute.</p> </li> <li> <p>A block of permissions will be printed in three triplets, representing the three permissions for each of the three sets:</p> <ul> <li>rwxrw-r-- means that the user can read, write, and execute; the group members can read and write; and anyone else can only read the file.</li> </ul> </li> <li> <p>To change permissions of a file, issue chmod with an argument specifying which set of permissions you want to modify, a + to add permissions or a - to remove permissions, and the permissions you want to modify.</p> <ul> <li> <p>This will add permissions for the user and group to read and write to secretDocument.txt.</p> <p><code>chmod ug+rw secretDocument.txt</code></p> </li> <li> <p>This will remove permissions for anyone who is not the user or the group to read or write to secretDocument.txt.</p> <p><code>chmod o-rw secretDocument.txt</code></p> </li> </ul> </li> </ul> <p>cat 'Concatenates' or prints the entire contents of a file to the screen (STDOUT).</p> <ul> <li>If you accidentally print out the contents of a gigantic file and want to kill it, you can do so with Ctrl-C.</li> </ul> <p>head Print the first n lines of a file.</p> <ul> <li> <p>The default behavior is n=10. This can be modified with the -n flag, as in</p> <p><code>head -n 100 secretDocument.txt</code></p> <p>which would print the first 100 lines of that file.</p> </li> </ul> <p>tail Print the last n lines of a file.</p> <ul> <li> <p>The default is n=10. This can also be modified with the -n flag, as above.</p> </li> <li> <p>Additionally, you can use tail to drop the first line of a file, like this:</p> <p><code>tail -n +2 secretDocument.txt</code></p> <p>which can be useful if you want to drop the line containing column headings, for example.</p> </li> </ul> <p>sort Print sorted output.</p> <ul> <li> <p>Useful flags;</p> <ul> <li> <p>-k Specifies the column number you want to sort by. You can specify multiple columns.</p> </li> <li> <p>-g Specifies that you want to sort using numbers based on scientific notation. This is generally how I sort numbers. There are many other flags for different variable types (alphabetic, date/time, etc).</p> </li> </ul> </li> </ul> <p>cut Prints a specified set of input data to the screen.</p> <ul> <li> <p>Useful flags:</p> </li> <li> <p>-f Specifies the column numbers you want to display. Separate multiple columns with commas, or type ranges of columns like '1-10'.</p> </li> </ul>"},{"location":"documentation/quick_starts/","title":"Quick Start Guides","text":""},{"location":"documentation/quick_starts/#high-performance-computing-at-ku","title":"High Performance Computing at KU","text":"<p>The Center for Research Computing has detailed guides to working on their research computing environments, which can be found on their website.</p>"},{"location":"documentation/quick_starts/#accessing-the-cluster","title":"Accessing the cluster","text":"<p>ssh Log in to a secure shell on a remote computer or server</p> <p>Log on to the cluster</p> <pre><code>ssh USERNAME@hpc.crc.ku.edu\n</code></pre>"},{"location":"documentation/quick_starts/#copying-files-tofrom-the-cluster","title":"Copying files to/from the cluster","text":"<p>scp Copy files between your computer and the server, or vice versa.</p> <p>Copy a file from your present working directory on your computer to your home directory on the cluster</p> <pre><code>scp myfile.txt USERNAME@hpc.crc.ku.edu:/home/USERNAME\n</code></pre> <p>Copy a file from your home directory on the cluster to your present working directory on your computer</p> <pre><code>scp USERNAME@hpc.crc.ku.edu:/home/USERNAME/a_file.txt ./\n</code></pre>"},{"location":"documentation/quick_starts/#nextflow-quick-start","title":"Nextflow Quick Start","text":"<p>We have prepared installed programs on the KU computing cluster to assist users in running Nextflow pipelines, which are accessible to users affiliated with the Genomic Data Science Core or with the Center for Genomics. However, Nextflow is intended to make workflows portable and can be used in many environments, and we provide some general guidance for getting started for non-KU users.</p>"},{"location":"documentation/quick_starts/#ku-genomic-data-science-core-users","title":"KU Genomic Data Science Core users","text":"<p>Include the following lines with the job submission script to use the pre-built Nextflow modules </p> <pre><code>module use --append /kuhpc/work/sjmac/observer/modules\nmodule load nextflow\n\nNXF_ANSI_SUMMARY=true\nNXF_ANSI_LOG=true\n</code></pre> <p>An example run script to run the nf-core RNAseq pipeline</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=nf_core_rnaseq\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=&lt;USERNAME&gt;@ku.edu\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task 1\n#SBATCH --time=24:00:00\n#SBATCH --partition=sjmac\n#SBATCH --mem=10G\n#SBATCH --output=%x_%A.out\n\nmodule use --append /kuhpc/work/sjmac/observer/modules\nmodule load nextflow\n\nNXF_ANSI_SUMMARY=true\nNXF_ANSI_LOG=true\n\nnextflow run nf-core/rnaseq \\\n-c /kuhpc/work/sjmac/observer/profiles/gdsc.config\n    --input sample_sheet.csv \\\n--genome BDGP6 \\\n--outdir output \\\n--aligner star_rsem \\\n--pseudo_aligner salmon \\\n-resume\n</code></pre>"},{"location":"documentation/quick_starts/#ku-center-for-genomics-users","title":"KU Center for Genomics users","text":"<p>Include the following lines with the job submission script to use the pre-built Nextflow modules </p> <pre><code>module use --append /kuhpc/work/kucg/observer/modules\nmodule load nextflow\n\nNXF_ANSI_SUMMARY=true\nNXF_ANSI_LOG=true\n</code></pre> <p>The GDSC workflows can be downloaded using Nextflow with the following command</p> <p>An example run script to run the nf-core RNAseq pipeline</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=nf_core_rnaseq\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=&lt;USERNAME&gt;@ku.edu\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task 1\n#SBATCH --time=24:00:00\n#SBATCH --partition=sjmac\n#SBATCH --mem=10G\n#SBATCH --output=%x_%A.out\n\nmodule use --append /kuhpc/work/sjmac/observer/modules\nmodule load nextflow\n\nNXF_ANSI_SUMMARY=true\nNXF_ANSI_LOG=true\n\nnextflow run nf-core/rnaseq \\\n-c /kuhpc/work/kucg/observer/profiles/kucg.config\n    --input sample_sheet.csv \\\n--genome BDGP6 \\\n--outdir output \\\n--aligner star_rsem \\\n--pseudo_aligner salmon \\\n-resume\n</code></pre>"},{"location":"documentation/quick_starts/#other-users","title":"Other users","text":"<ol> <li> <p>Install Nextflow following the directions on the Nextflow website</p> <ul> <li>Note: Nextflow can be installed in any location, but for the purposes of this documentation it is assumed to be located in the home directory (e.g. <code>~/nextflow</code>)</li> </ul> </li> <li> <p>Run a small test job</p> </li> </ol> <p>To test that everything is working correctly, you can run a simple test program with the command:</p> <pre><code>~/nextflow run hello\n</code></pre> <p>If everything is loaded properly, you should output that resembles the following:</p> <pre><code>N E X T F L O W  ~  version 23.04.3\nLaunching `https://github.com/nextflow-io/hello` [confident_picasso] DSL2 - revision: 1d71f857bb [master]\nexecutor &gt;  local (4)\n[5a/e0c8dc] process &gt; sayHello (2) [100%] 4 of 4 \u2714\nHello world!\n\nBonjour world!\n\nHola world!\n\nCiao world!\n</code></pre> <ol> <li>Download the Nextflow pipeline of interest</li> </ol> <p>Download the nf-core RNAseq pipeline</p> <pre><code>~/nextflow pull nf-core/rnaseq\n</code></pre> <ol> <li>Run the Nextflow pipeline of interest</li> </ol> <p>Example run command for nf-core RNAseq pipeline</p> <pre><code>~/nextflow run nf-core/rnaseq \\\n--input sample_sheet.csv \\\n--genome BDGP6 \\\n--outdir output \\\n--aligner star_rsem \\\n--psudo_aligner salmon \\\n--resume\n</code></pre>"},{"location":"documentation/slow_starts/","title":"Detailed Guides","text":""},{"location":"documentation/slow_starts/#ssh-configuration","title":"SSH Configuration","text":"<p>Normally, accessing the cluster via <code>ssh</code> and <code>scp</code> requires you to enter your username and password each time. However, it is possible to create a configuration profile on your local computer that will make this process more efficient. Using a text editor, you can create or edit the file  <code>~/.ssh/config</code> to include the following lines:</p> <p>Editing ~/.ssh/config</p> <pre><code>Host cluster\n    HostName hpc.crc.ku.edu\n    User USERNAME\n</code></pre> <p>Where <code>USERNAME</code> is your own username. The host <code>cluster</code> is arbitrary, you any name you like. After editing or creating this file, you can now use less complicated <code>ssh</code> and <code>scp</code> commands to log in or copy files.</p> <p>Log in with an SSH config file</p> <pre><code>ssh cluster\n</code></pre> <p>Copy a file from your present working directory on your computer to your home directory on the cluster</p> <pre><code>scp myfile.txt cluster:/home/USERNAME\n</code></pre>"},{"location":"documentation/slow_starts/#working-interactively-on-the-cluster","title":"Working interactively on the cluster","text":"<p>When you initially log in to the cluster you are on a \"login node,\" which is designed for handling logins and basic tasks such as submitting  job scripts to the resource scheduler, but is not designed to handle resource-intensive analysis work. When you are on a login node,  the command prompt will look like this:</p> <pre><code>[USERNAME@submit1 ~]$\n</code></pre> <p>If you want to work interactively on the cluster you must first request a \"compute node,\" which lets you specify the number of CPUs, the amount of memory, etc. To request a compute node, use a command such as the following:</p> <pre><code>srun --nodes=1 --mem=128G --time=01:00:00 --partition=sixhour --pty /bin/bash -l\n</code></pre> <p>This command will allocate the resources you've requested, which in this case is a single node, with 128GB of memory, for 1 hour, using the \"sixhour\" community cluster. When you enter this command, you should see something like this:</p> <pre><code>srun: job 55741230 queued and waiting for resources\nsrun: job 55741230 has been allocated resources\n</code></pre> <p>And you should notice that the command prompt has changed, and reflects that you are working on a compute node:</p> <pre><code>[USERNAME@r11r12n03 ~]$\n</code></pre> <p>The program <code>crctool</code> allows you to see what resources you have available to request, either for interactive sessions or in job submission scripts</p>"},{"location":"documentation/slow_starts/#running-nextflow-pipelines-on-the-cluster","title":"Running Nextflow pipelines on the cluster","text":"<p>We have prepared some software modules to help make it more convenient to run pipelines on the cluster. In order to use these modules, you need to run the following commands:</p> <pre><code>module use --append /kuhpc/work/sjmac/observer/modules\nmodule load nextflow\n</code></pre> <p>The first line adds the set of modules the GDSC has prepared to your profile. The second line loads the module <code>nextflow</code>, which also load a few required programs such as <code>java</code> and <code>singularity</code>.</p> <p>Note: these commands need to be run every time you log in, or in the body of any script that you run, in order for the modules to be loaded properly. In order to have this happen automatically, you can add those lines to the file <code>~/.bashrc</code>, which is used to load various programs every time you log into the cluster.</p> <p>To test that everything is working correctly, you can run a simple test program with the command:</p> <p><code>nextflow run hello</code></p> <p>If everything is loaded properly, you should see the following output</p> <pre><code>N E X T F L O W  ~  version 23.04.3\nLaunching `https://github.com/nextflow-io/hello` [confident_picasso] DSL2 - revision: 1d71f857bb [master]\nexecutor &gt;  local (4)\n[5a/e0c8dc] process &gt; sayHello (2) [100%] 4 of 4 \u2714\nHello world!\n\nBonjour world!\n\nHola world!\n\nCiao world!\n</code></pre>"},{"location":"documentation/slow_starts/#setting-up-long-running-jobs-on-the-cluster","title":"Setting up long-running jobs on the cluster","text":"<p>Rather than running programs directly from the command line, as we've been doing so far, it often makes sense to prepare scripts that can be submitted to the CRC job scheduler to run in the background. This has several benefits:</p> <ol> <li> <p>Jobs run this way aren't tied to your login session, so if your computer goes to sleep or you get disconnected, they will continue running until they're completed.</p> </li> <li> <p>You can request much larger amounts of computing resources this way, and so it can make your job run more efficiently.</p> </li> <li> <p>The job scheduler can notify you via e-mail when your job completes (successfully or otherwise) so you don't need to keep a constant eye on things.</p> </li> </ol> <p>An example job script to run the simple Nextflow test job above looks like this:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=nextflow_test\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=&lt;YOUR_EMAIL_ADDRESS&gt;\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task 1\n#SBATCH --time=24:00:00\n#SBATCH --partition=sjmac\n#SBATCH --mem=10G\n#SBATCH --output=%x_%A.out\n\nmodule use --append /kuhpc/work/sjmac/observer/modules\nmodule load nextflow\n\nNXF_ANSI_SUMMARY=true\nNXF_ANSI_LOG=true\n\nnextflow run hello\n</code></pre> <p>The lines beginning with <code>#SBATCH</code> are all directions to the job submission system to specify how much memory you want (<code>#SBATCH --mem=</code>), how much time you want to allocate for the job (<code>#SBATCH --time</code>), and may other. More detail about job submission scripts is available on on the CRC documentation website.</p> <p>Try using the above example to make a script called <code>run_script.sh</code> on the cluster. Modify the line  <code>#SBATCH --mail-user=&lt;YOUR_EMAIL_ADDRESS&gt;</code> to replace <code>&lt;YOUR_EMAIL_ADDRESS&gt;</code> with your actual email address (e.g. <code>#SBATCH --mail-user=gdsc@ku.edu</code>).</p> <p>sbatch Batch job submission to the Slurm job scheduler on the cluster.</p> <p>Submit a job script to run an analysis</p> <pre><code>sbatch run_script.sh\n</code></pre> <p>squeue Provide the status of running jobs on the cluster</p> <p>Check the status of jobs that you have submitted</p> <pre><code>squeue -u USERNAME\n</code></pre> <p>The output should look like this:</p> <pre><code>JOBID     PARTITION     NAME     USER     ST     TIME     NODES     NODELIST(REASON) \n55741582_1     sjmac     treemix     observer     R     20:27:52     1     r11r12n02\n55741582_2     sjmac     treemix     observer     R     20:27:52     1     r06r28n03\n55741582_3     sjmac     treemix     observer     R     20:27:52     1     r15r18n02\n55741582_4     sjmac     treemix     observer     R     20:27:52     1     r15r18n02\n55741582_5     sjmac     treemix     observer     R     17:31:00     1     r11r12n02   \n</code></pre> <p>The column \"ST\" refers to the state of the job. The most common states are:</p> <ul> <li> <p>PD: Pending</p> </li> <li> <p>R: Running</p> </li> <li> <p>CG: Completing</p> </li> </ul> <p>scancel Cancel a job that is running on the cluster</p> <p>Cancel a specific job with the JobID (the first column of the squeue output)</p> <pre><code>scancel 0000001\n</code></pre> <p>Cancel all jobs that you have submitted that are running or pending</p> <pre><code>scancel -u USERNAME\n</code></pre> <p>This is useful if you make a mistake with a big batch job that submits many subjobs</p>"},{"location":"documentation/workflows/","title":"Workflow Documentation:","text":""},{"location":"documentation/workflows/#ku-gdsc-developed-workflows","title":"KU GDSC developed workflows:","text":"<ol> <li>Microbial RNA-seq: <code>microbial_rnaseq</code> </li> </ol>"},{"location":"documentation/workflows/#quick-start-for-ku-users","title":"Quick Start for KU Users","text":"<p>An example run script to run the microbial RNAseq pipeline</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=microbial_rnaseq\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=&lt;USERNAME&gt;@ku.edu\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task 1\n#SBATCH --time=24:00:00\n#SBATCH --partition=sjmac\n#SBATCH --mem=10G\n#SBATCH --output=%x_%A.out\n\nmodule use --append /kuhpc/work/sjmac/observer/modules\nmodule load nextflow\n\nNXF_ANSI_SUMMARY=true\nNXF_ANSI_LOG=true\n\nnextflow run ku-gdsc/workflows \\\n-r main \\\n--workflow microbial_rnaseq \\\n--fasta /PATH/TO/FASTA \\\n--gff /PATH/TO/GFF \\\n--sample_folder /PATH/TO/FASTQ/FILES \\\n--pubdir /PATH/TO/OUTPUT/RESULTS \\\n-w /PATH/TO/WORK/DIRECTORY \\\n--strandedness reverse_stranded \\\n-profile gdsc \\\n-resume\n</code></pre> <p>Notes: </p> <ol> <li> <p>This example runs the microbial RNA-seq workflow. It requires the specification of a reference FASTA, a GFF3, and a folder of FASTQ files (not provided here). See the full workflow documentation for additional detail</p> </li> <li> <p>For all pipelines, running the flag <code>--help</code> (e.g, <code>~/nextflow &lt;PATH&gt;/&lt;TO&gt;/main.nf --workflow microbial_rnaseq --help</code>) will print help documentation for that pipeline and quit.</p> </li> </ol>"},{"location":"documentation/workflows/microbial_rnaseq/","title":"Microbial RNA-seq","text":""},{"location":"documentation/workflows/microbial_rnaseq/#summary-of-methods","title":"Summary of methods","text":"<ul> <li>Index generated with RSEM</li> <li>Filter and trim reads with fastp</li> <li>Quality checking of reads with FastQC</li> <li>Reads mapped to reference and quantified with RSEM</li> <li>Alignment post-processing and QC with Picard</li> <li>Aggregation of QC tables using MultiQC</li> </ul>"},{"location":"documentation/workflows/microbial_rnaseq/#parameters","title":"Parameters","text":""},{"location":"documentation/workflows/microbial_rnaseq/#required-parameters","title":"Required parameters:","text":"<ul> <li> <p><code>--pubdir</code> </p> <ul> <li>Default: <code>/&lt;PATH&gt;</code> </li> <li>Description: The directory that the saved outputs will be stored.</li> </ul> </li> <li> <p><code>-w</code></p> <ul> <li>Default: <code>/&lt;PATH&gt;</code></li> <li>The directory that all intermediary files and nextflow processes utilize. This directory can become quite large. This should be a location on scratch space or other directory with ample storage.</li> </ul> </li> <li> <p><code>--sample_folder</code> </p> <ul> <li>Default: <code>/&lt;PATH&gt;</code></li> <li>The path to the folder that contains all the samples to be run by the pipeline. The files in this path can also be symbolic links.</li> </ul> </li> <li> <p><code>--strandedness</code></p> <ul> <li>Default: <code>NA</code></li> <li>Supported options are <code>reverse_stranded</code>, <code>forward_stranded</code>, <code>non_stranded</code></li> </ul> </li> <li> <p><code>--concat_lanes</code></p> <ul> <li>Default: <code>false</code></li> <li>Options: <code>false</code> and <code>true</code>. Default: <code>false</code>. If this boolean is specified, FASTQ files will be concatenated by sample. Used in cases where samples are divided across individual sequencing lanes.</li> </ul> </li> <li> <p><code>--fasta</code></p> <ul> <li>Default: <code>/&lt;PATH&gt;</code></li> <li>Path to the reference genome in FASTA format</li> </ul> </li> <li> <p><code>--gff</code></p> <ul> <li>Default: <code>/&lt;PATH&gt;</code></li> <li>Path to the annotation for the reference genome in GFF3 format</li> </ul> </li> <li> <p><code>--read_type</code></p> <ul> <li>Default: <code>PE</code></li> <li>Comment: Type of reads: paired end (PE) or single end (SE).</li> </ul> </li> </ul>"},{"location":"documentation/workflows/microbial_rnaseq/#fastp-filtering-paramenters","title":"fastp filtering paramenters:","text":"<ul> <li> <p><code>--quality_phred</code></p> <ul> <li>Default: 15</li> <li>Quality score threshold.</li> </ul> </li> <li> <p><code>--unqualified_perc</code></p> <ul> <li>Default: 40</li> <li>Percent threshold of unqualified bases to pass reads.</li> </ul> </li> </ul>"},{"location":"documentation/workflows/microbial_rnaseq/#pipeline-default-outputs","title":"Pipeline Default Outputs","text":"Naming Convention Description <code>rsem.merged.gene_counts.tsv</code> RSEM-generated gene-level raw counts merged across all samples <code>rsem.merged.gene_tpm.tsv</code> RSEM-generated gene-level TPM counts merged across all samples <code>rsem.merged.isoform_counts.tsv</code> RSEM-generated isoform-level raw counts merged across all samples <code>rsem.merged.isoform_tpm.tsv</code> RSEM-generated isoform-level TPM counts merged across all samples <code>microbial_rnaseq_report.html</code> Nextflow autogenerated report <code>multiqc/</code> MultiQC report summarizing quality metrics across all samples in the run <code>${sampleID}/bam/${sampleID}.genome.bam</code> RSEM-generated alignment of reads to the reference genome <code>${sampleID}/bam/${sampleID}.transcript.bam</code> RSEM-generated alignment of reads to the reference transcriptome <code>${sampleID}/${sampleID}.genes.results</code> RSEM-generated quantification of gene-level count abundances <code>${sampleID}/${sampleID}.isoforms.results</code> RSEM-generated quantification of transcript-level count abundances <code>trace.txt</code> Nextflow trace of processes"},{"location":"documentation/workflows/microbial_rnaseq/#workflow-validation","title":"Workflow Validation","text":"<p>The genome assembly and annotation for Enterococcus faecalis V583 were downloaded from NCBI and used to generate simulated RNA-seq reads with the function  <code>simReads</code> in the R package Rsubread. To test the performance of the workflow, we generated three sets of \"control\" paired-end reads, in which the TPM values were assigned based on random sampling, and three sets of \"case\" control reads, in which 50 genes were randomly selected to have substantially higher TPM values.</p> <p>We then ran the workflow using the E. faecalis reference to generate indices, and compared the raw count values quantified by RSEM with the \"true\" count values expected from the simulated reads generated by Rsubread.</p> <p></p> <p>The dashed line represents a 1:1 reference, and the deviation of the  counts quantified by RSEM from the \"True\" counts is due to the quality filtering of the read data, as the reads were simulated to include sequencing error.</p> <p>We then performed differential expression analyses on both the true counts and the counts estimated by RSEM in the workflow.</p> <p></p> <p>Again, the dashed line is a 1:1 reference. The red points are those which would be scored as \"significant\" with standard thresholds of | log<sub>2</sub> fold-change | &gt; 2 and FDR-adjusted P-values &lt; 0.01.</p>"}]}